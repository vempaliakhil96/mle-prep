{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Get word level embeddings, idea to generate these embeddings is to read through large corupus of texts like wikipedia , news, books etc and generate embeddings for words based on the context in which they appear.\n",
    "\n",
    "### Continuous Bag of Words (CBOW)\n",
    "\n",
    "Predict the current word based on the surrounding words\n",
    "\n",
    "$$ Loss = -log(p(w_t|w_{t-n},...,w_{t-1},w_{t+1}...,w_{t+n})) $$\n",
    "\n",
    "### Skipgram Model\n",
    "\n",
    "Predict the surrounding words based on the current word\n",
    "\n",
    "$$ Loss = -log(p(w_{t-n},...,w_{t-1},w_{t+1}...,w_{t+n}|w_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "GloVe is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.\n",
    "\n",
    "$$Loss = \\sum_{i,j=1}^{V} f(P_{ij})(w_i^T\\tilde{w}_j + b_i + \\tilde{b}_j - log(P_{ij}))^2$$\n",
    "\n",
    "where $P_{ij}$ is the probability of word $i$ appearing in the context of word $j$. $f$ is a weighting function that assigns less weight to rare words. $w_i$ and $\\tilde{w}_j$ are the word vectors and $b_i$ and $\\tilde{b}_j$ are the bias terms. $V$ is the vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "Use an LLM to generate a \"fake\" document for the input query, embed this fake document and \n",
    "vector search this embedding over the document embeddings in the corpus.\n",
    "![](../assets/hyde-embedding.png)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
