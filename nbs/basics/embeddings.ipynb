{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Get word level embeddings, idea to generate these embeddings is to read through large corupus of texts like wikipedia , news, books etc and generate embeddings for words based on the context in which they appear.\n",
    "\n",
    "### Continuous Bag of Words (CBOW)\n",
    "\n",
    "Predict the current word based on the surrounding words\n",
    "\n",
    "$$ Loss = -log(p(w_t|w_{t-n},...,w_{t-1},w_{t+1}...,w_{t+n})) $$\n",
    "\n",
    "### Skipgram Model\n",
    "\n",
    "Predict the surrounding words based on the current word\n",
    "\n",
    "$$ Loss = -log(p(w_{t-n},...,w_{t-1},w_{t+1}...,w_{t+n}|w_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "Use an LLM to generate a \"fake\" document for the input query, embed this fake document and \n",
    "vector search this embedding over the document embeddings in the corpus.\n",
    "![](../assets/hyde-embedding.png)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
