{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "This model tries to find the best line that fits the data. The line is defined by the equation `y = mx + b`, where `m` is the slope of the line and `b` is the y-intercept. The model tries to find the best values for `m` and `b` that minimize the error between the predicted values and the actual values. Usually we use OLS (Ordinary Least Squares) to find the best values for `m` and `b`.\n",
    "\n",
    "## SVM\n",
    "\n",
    "In this model we try to find the best hyperplane that separates the data into two classes. The hyperplane is defined by the equation `w^T * x + b = 0`, where `w` is the normal vector to the hyperplane and `b` is the bias. The model tries to find the best values for `w` and `b` that maximize the margin between the two classes. If the data is not linearly separable, we can use the kernel trick to transform the data into a higher dimensional space where it is linearly separable. Kernel functions can be linear, polynomial, radial basis function (RBF).\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "This model is inspired by the human brain. It consists of layers of neurons that are connected to each other. Each neuron receives some input, applies an activation function to it, and passes the output to the next layer. The model tries to find the best weights for the connections between the neurons that minimize the error between the predicted values and the actual values. We can have different architectures for neural networks, such as feedforward, convolutional, recurrent, etc.\n",
    "\n",
    "## Naive Bayes Classifier\n",
    "\n",
    "This model is based on Bayes' theorem. It assumes that the features are independent of each other. The model calculates the probability of each class given the features and selects the class with the highest probability. We can have different types of Naive Bayes classifiers, such as Gaussian, Multinomial, Bernoulli. More formally, the model calculates the probability of each class given the features using the formula: $P(class|features) = P(class) * P(features|class) / P(features)$. We select the class with the highest probability.\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "This model is used for binary classification problems. It tries to find the best line that separates the data into two classes. The line is defined by the equation `y = mx + b`, where `m` is the slope of the line and `b` is the y-intercept. The model applies the sigmoid function to the output of the line to get the probability of the positive class. The model tries to find the best values for `m` and `b` that maximize the likelihood of the data. Formula for sigmoid function: \n",
    "\n",
    "$f(x) = 1 / (1 + e^{-x})$.\n",
    "\n",
    "## K-Nearest Neighbors\n",
    "\n",
    "This model is based on the idea that similar data points tend to be close to each other. The model stores all the data points and their labels in memory. When we want to make a prediction for a new data point, the model finds the `k` nearest neighbors to the new data point and selects the majority class among them. We can use different distance metrics to find the nearest neighbors, such as Euclidean, Manhattan, Minkowski, etc.\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "This model is based on a tree-like structure where each node represents a feature, each branch represents a decision based on that feature, and each leaf node represents the outcome. The model tries to find the best feature and the best value for that feature that splits the data into two classes. We can use different criteria to measure the impurity of the data, such as Gini impurity, entropy, misclassification error. The model keeps splitting the data until it reaches a stopping criterion, such as maximum depth, minimum samples per leaf, etc. \n",
    "\n",
    "## Random Forest\n",
    "\n",
    "This model is an ensemble of decision trees. It creates multiple decision trees using random subsets of the data and random subsets of the features. Each tree makes a prediction, and the final prediction is the majority vote among all the trees. This helps to reduce overfitting and improve the generalization of the model. We can use different criteria to measure the impurity of the data, Gini impurity, entropy, misclassification error.\n",
    "\n",
    "Formula for Gini impurity:\n",
    "\n",
    "$Gini = 1 - \\sum_{i=1}^{n} p_i^2$. where $p_i$ is the probability of class `i`.\n",
    "\n",
    "## Gradient Boosting\n",
    "\n",
    "This model is an ensemble of weak learners, usually decision trees. It creates multiple decision trees sequentially, where each tree tries to correct the errors made by the previous trees. The model combines the predictions of all the trees to make the final prediction. We can use different loss functions to measure the error, such as squared error, absolute error, etc. We can use different boosting algorithms, such as AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "## K-Means\n",
    "\n",
    "This model is used for clustering problems. It tries to find `k` clusters in the data. The model initializes `k` centroids randomly and assigns each data point to the nearest centroid. Then it updates the centroids by taking the mean of all the data points assigned to that centroid. The model repeats this process until the centroids do not change significantly. We can use different distance metrics to find the nearest centroid, such as Euclidean, Manhattan, Minkowski, etc.\n",
    "\n",
    "## DBSCAN\n",
    "\n",
    "This model is used for clustering problems. It tries to find clusters of arbitrary shapes in the data. The model takes two parameters, `epsilon` and `min_samples`. It starts with a random data point and finds all the data points within `epsilon` distance from it. If the number of data points is greater than `min_samples`, it forms a cluster. Then it expands the cluster by finding all the data points within `epsilon` distance from the border points. The model repeats this process until all the data points are assigned to a cluster.\n",
    "\n",
    "## PCA\n",
    "\n",
    "This model is used for dimensionality reduction. It tries to find the principal components of the data, which are the directions along which the data varies the most. The model projects the data onto the principal components, reducing the dimensionality of the data. We can select the number of principal components based on the amount of variance we want to retain in the data. The model can be used for visualization, noise reduction, feature extraction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
