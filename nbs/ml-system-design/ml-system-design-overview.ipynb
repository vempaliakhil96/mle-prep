{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML System Design Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key steps in ML system design\n",
    "\n",
    "Types of questions that are asked in ML system design interviews are:\n",
    "- Build a system that shows relevant ads for search engines.\n",
    "- Recommend movies to a user on Netflix.\n",
    "- Extract all persons, locations, and organizations from a given corpus of documents.\n",
    "\n",
    "Key points of discsussion in ML system design are:\n",
    "\n",
    "1. Problem Setting\n",
    "2. Understanding the scale and latency requirements\n",
    "    1. Latency\n",
    "    2. Scale\n",
    "3. Defining metrics\n",
    "    1. Offline metrics\n",
    "    2. Online metrics\n",
    "4. Architecture discussion\n",
    "    1. Architecting for scale\n",
    "5. Offline model building and evaluation\n",
    "6. Online model execution and evaluation\n",
    "7. Iterative model improvements\n",
    "\n",
    "\n",
    "## Problem Setting\n",
    "\n",
    "Usually the problem statement by the interviewer might be broad and it is important to ask clarifying questions to understand the problem better. Ask clarifying questions till you feel that all aspects of the problem are clear to you and convey your understanding to the interviewer so that both of you are on the same page.\n",
    "\n",
    "Some questions that you might ask are:\n",
    "\n",
    "- What is the objective of the system?\n",
    "- Does it work like XXX ? (where XXX is a similar system that you know of)\n",
    "- How can one obtain such inputs?\n",
    "- How will the output be consumed?\n",
    "\n",
    "Remember that post this step, you should have a fair idea of the problem statement and the objective of the system.\n",
    "\n",
    "## Understanding the scale and latency requirements\n",
    "\n",
    "This can be useful when identifying where and how caching might need to be implemented.\n",
    "\n",
    "### Latency\n",
    "Understanding latency requirements will help in indentifying the correct ML solution / model to use. \n",
    "\n",
    "### Scale\n",
    "Scale will help us understand how many requests we can expect for the system. \n",
    "\n",
    "## Defining metrics\n",
    "see [here](./ml-metrics.ipynb)\n",
    "\n",
    "### Offline metrics\n",
    "\n",
    "These metrics are used when the model is being built and evaluated. In supervised setting these metrics would be calculated on the validation set, where we will have the ground truth. Some example metrics are:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- AUC-ROC\n",
    "\n",
    "### Online metrics\n",
    "\n",
    "These metrics are used when the model is being run in the production environment. These are basically used to monitor the performance of the model in real time. Mainly to ensure that the model is performing as expected. Some example metrics are:\n",
    "- Click through rate\n",
    "- Conversion rate\n",
    "- Bound rate\n",
    "\n",
    "## Architecture discussion\n",
    "\n",
    "Discussion on how the system will be built given the requirements and scale. Here we will talk the various components of the system and what aspects of the requirements or performance will each of the component address. \n",
    "\n",
    "### Architecting for scale\n",
    "\n",
    "To handle large scale, we might need to have a funneled approach in our ML system. With each layer filtering out irreleavant samples/examples and increasing in complexity. In that way, the most complex model (which will ususally be the most computationally expensive) will only be run on a small subset of the data. Another way to handle scale for a different use case might be to perform batch predictions. This would ensure that out resources are used optimally and we are able to handle multiple requests at the same time. When making batch predictions, we need to consider what is an acceptable linger time for a request, as this will enable us to collect multiple requests and make predictions in a batch. \n",
    "\n",
    "## Offline model building and evaluation\n",
    "\n",
    "This is the step where we actually build the most optimal model. For model building the first thing required is training data. \n",
    "\n",
    "### Training data\n",
    "\n",
    "1. **Human annotated data** : We can have crowd sourced data or data annotated by experts. If the task is generic, we can also utilize pre-existing datasets.\n",
    "2. **User Interaction data** : User interaction data could be used in creating personalisation models, where we can use user interactions to understand preferences and recommend items accordingly. \n",
    "3. **Synthetic data** : Synthetic data can be used when we do not have enough data. This can be generated using techniques like data augmentation. Sometimes we can also use an existing pretriained more complex model to generate this synthetic data, and use that data to train a smaller more robust model.\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that can be used in the model. This is a crucial step as the model's performance is highly dependent on the features used. Some common feature engineering techniques are:\n",
    "- One hot encoding\n",
    "- Normalization\n",
    "- Standardization\n",
    "- Binning\n",
    "- Missing value imputation\n",
    "\n",
    "### Model building\n",
    "\n",
    "Based on offline evaluation metrics, we can train multiple models and compare / contrast their performance. Here the main choice of would be the model architecture. Usually, we would use an random forest or a GBM model for tabular data, and a CNN or Transformer for image and text data. Random forests are good for tabular data because they are robust to outliers and can handle missing data well. GBM models are good for tabular data because they are able to capture non-linear relationships in the data. CNNs are good for image data because they are able to capture spatial relationships in the data. Transformers are good for text data because they are able to capture the sequential relationships in the data.\n",
    "\n",
    "## Online model execution and evaluation\n",
    "\n",
    "After the model is trained and deployed, we need to monitor its performance using online metrics. It is also a good idea to store model predictions as it helps create an organic training data. This data can be used to retrain the model and improve its performance. If possible, we can also incorporate feedback from the users to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
